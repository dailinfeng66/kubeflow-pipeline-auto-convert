# 分布式训练

## 并行策略

有两种深度学习模型并行的方式：

1. 模型并行
2. 数据并行

### 模型并行



模型并行性是指将模型逻辑上分成若干部分（即，一部分中的一些层，另一部分中的一些部分），然后将它们放置在不同的硬件/设备上。 虽然将部件放置在不同的设备上确实在执行时间方面具有优势（数据的异步处理），但是通常采用它来避免存储器约束。 具有大量参数的模型由于高内存占用而难以适合单个系统，因此受益于此类策略。



### 数据并行

另一方面，数据并行性是指通过位于不同硬件/设备上的相同网络的多个副本来处理多个（技术上批量）数据。 与模型并行性不同，每个副本可能是整个网络而不仅仅是其中的一部分。 您可能已经猜到，这种策略可以随着数据量的增加而扩展。 但是，由于整个网络必须驻留在单个设备上，因此无法帮助具有高内存占用量的模型。 
